Exercise 6.8
------------

Setting up
```{r results="hide", message=FALSE}
library(leaps)
library(glmnet)
x = rnorm(100)
y = 1 + 3*x + 4*x^2 - 2*x^3 + rnorm(100)
XY = data.frame(x, y)
XYmat = model.matrix(y~poly(x, 10, raw = TRUE), data = XY)
```

From the R session:
```{r}
predict.regsubsets = function(object, newdata, id, ...) {
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  mat[, names(coefi)] %*% coefi
}
```

Best subset selection:
```{r}
regfit.full = regsubsets(y~1+poly(x, 10, raw = TRUE), data = XY, nvmax = 11)
which.max(summary(regfit.full)$adjr2)
which.min(summary(regfit.full)$cp)
which.min(summary(regfit.full)$bic)
```

The best model is that with 3 or 4 parameters. The coefficient for the fourth parameter is very small, however, and the two models are very similar:
```{r}
pred3 = data.frame(cbind(x, predict(regfit.full, XY, 3)))[order(x),]
pred4 = data.frame(cbind(x, predict(regfit.full, XY, 4)))[order(x),]
plot(x, y)
lines(pred3, col = "red")
lines(pred4, col = "blue", lty = 3)
```

Forward stepwise selection:
```{r}
regfit.forw = regsubsets(y~1+poly(x, 10, raw = TRUE), data = XY, nvmax = 11, method = "forward")
which.max(summary(regfit.forw)$adjr2)
which.min(summary(regfit.forw)$cp)
which.min(summary(regfit.forw)$bic)
```

We now find models with 5 and 6 parameters to give the best fit, but again the additional parameters are very small, and we again get a very similar fit:
```{r}
pred5 = data.frame(cbind(x, predict(regfit.forw, XY, 5)))[order(x),]
pred6 = data.frame(cbind(x, predict(regfit.forw, XY, 6)))[order(x),]
plot(x, y)
lines(pred5, col = "red")
lines(pred6, col = "blue", lty = 3)
```

Backward stepwise selection yields similar results:
```{r}
regfit.back = regsubsets(y~1+poly(x, 10, raw = TRUE), data = XY, nvmax = 11, method = "forward")
which.max(summary(regfit.back)$adjr2)
which.min(summary(regfit.back)$cp)
which.min(summary(regfit.back)$bic)

pred5 = data.frame(cbind(x, predict(regfit.back, XY, 5)))[order(x),]
pred6 = data.frame(cbind(x, predict(regfit.back, XY, 6)))[order(x),]
plot(x, y)
lines(pred5, col = "red")
lines(pred6, col = "blue", lty = 3)
```

Lasso method:
```{r}
fit.lasso = glmnet(XYmat, y)
cv.lasso = cv.glmnet(XYmat, y)
plot(cv.lasso)
```

Lasso chooses a model with 5 parameters, but again the coefficients for the higher-order parameters are small:
```{r}
coef(cv.lasso)
```




We now do the whole thing again for the second model:
```{r}
y2 = 1 + 3*x^7 + rnorm(100)
XY2 = data.frame(x, y2)
XY2mat = model.matrix(y2~poly(x, 10, raw = TRUE), data = XY2)
```

Best subset selection now yields some different results:
```{r}
regfit.full = regsubsets(y2~1+poly(x, 10, raw = TRUE), data = XY2, nvmax = 11)
which.max(summary(regfit.full)$adjr2)
which.min(summary(regfit.full)$cp)
which.min(summary(regfit.full)$bic)
```

Forward stepwise selection:
```{r}
regfit.forw = regsubsets(y2~1+poly(x, 10, raw = TRUE), data = XY2, nvmax = 11, method = "forward")
which.max(summary(regfit.forw)$adjr2)
which.min(summary(regfit.forw)$cp)
which.min(summary(regfit.forw)$bic)
```

Backward stepwise selection:
```{r}
regfit.back = regsubsets(y2~1+poly(x, 10, raw = TRUE), data = XY2, nvmax = 11, method = "forward")
which.max(summary(regfit.back)$adjr2)
which.min(summary(regfit.back)$cp)
which.min(summary(regfit.back)$bic)
```

Lasso method correctly chooses a model with only one parameter:
```{r}
fit.lasso = glmnet(XY2mat, y2)
cv.lasso = cv.glmnet(XY2mat, y2)
plot(cv.lasso)
coef(cv.lasso)
```